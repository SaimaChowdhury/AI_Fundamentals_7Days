**ğŸ“˜ Day 3 â€“ Gradient Descent & Optimization**

ğŸ“– Concepts Learned
- Gradient Descent: An optimization algorithm that helps models learn by minimizing errors step by step.
- Error Function (Loss): Measures how wrong the model is (e.g., Mean Squared Error).
- Learning Rate (\alpha ): Controls the step size in gradient descent.
- Convergence: When the algorithm reaches the minimum error.
- Local vs Global Minimum: Local = small dip, Global = lowest dip (best solution).

ğŸ§ª Examples & Practice
- Implemented Gradient Descent for Linear Regression from scratch.
- Experimented with different learning rates to see effects on convergence.
- Plotted the error curve over iterations to visualize how the model improves.

ğŸ”‘ Key Takeaways
- Gradient Descent is like walking downhill blindfolded â€” small steps lead to the lowest point.
- The learning rate is crucial: too small = slow, too large = unstable.
- RMSE is a useful metric to measure how well the model fits.
- Visualizing the error curve makes the learning process clear.

ğŸ“Š Metrics Learned
- MSE (Mean Squared Error) â†’ measures average squared error.
- RMSE (Root Mean Square Error) â†’ square root of MSE, easier to interpret.

ğŸ“ Reflections
- Gradient Descent helped me understand how models actually learn.
- Playing with learning rates showed the importance of balance.
- Visualizing convergence gave me confidence in the process.
- This day felt like a breakthrough in connecting math with coding practice.
