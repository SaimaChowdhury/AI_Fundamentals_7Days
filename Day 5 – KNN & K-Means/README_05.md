**ğŸ“˜ Day 6 â€“ KNN & Kâ€‘Means**

ğŸ“– Concepts Learned
- Kâ€‘Nearest Neighbors (KNN):
- A classification algorithm that predicts labels based on the majority vote of k nearest neighbors.
- Relies on distance metrics (commonly Euclidean).
- Sensitive to choice of k and scaling of features.
- Kâ€‘Means Clustering:
- An unsupervised algorithm that groups data into k clusters.
- Each cluster has a centroid (mean point).
- Iteratively assigns points to nearest centroid and updates centroids until stable.

ğŸ§ª Examples & Practice
- Implemented KNN classifier on the Iris dataset.
- Experimented with different values of k (3, 5, 7) and observed accuracy changes.
- Applied Kâ€‘Means clustering on unlabeled data (e.g., customer segmentation).
- Visualized clusters with scatter plots.

ğŸ”‘ Key Takeaways
- KNN is simple yet effective, but computationally heavy for large datasets.
- Choosing the right k is crucial â€” too small = noisy, too large = oversimplified.
- Kâ€‘Means discovers hidden structure without labels, but requires choosing k beforehand.
- Visualization makes clustering results intuitive and easy to interpret.

ğŸ“Š Metrics Learned
- KNN: Accuracy, Precision, Recall.
- Kâ€‘Means: Inertia (sum of squared distances), Silhouette Score.

ğŸ“ Reflections
- KNN felt like asking neighbors for advice â€” simple but surprisingly powerful.
- Kâ€‘Means was like sorting marbles into buckets â€” satisfying to see clusters form.
- Both algorithms rely on distance and similarity as their foundation.
- Visualizing results made the concepts click instantly.
