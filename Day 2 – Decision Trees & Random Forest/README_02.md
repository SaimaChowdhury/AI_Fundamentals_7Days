ğŸ“˜ **Day 2 â€“ Decision Trees & Random Forests**

ğŸ“– Concepts Learned
- Decision Trees: Models that split data step by step, like a flowchart of questions.
- Entropy: Measures disorder or impurity in data.
- Information Gain: Measures how much uncertainty is reduced after a split.
- Random Forests: An ensemble of many decision trees working together to improve accuracy and reduce overfitting.

ğŸ§ª Examples & Practice
- Built a Decision Tree to classify Titanic survival based on Age and Fare.
- Visualized the tree splits to see how questions are asked.
- Implemented a Random Forest on the same dataset.
- Compared accuracy between a single tree and a random forest.

ğŸ”‘ Key Takeaways
- Decision trees are intuitive â€” like asking questions step by step.
- Entropy and information gain decide the â€œbestâ€ question to ask at each split.
- Random forests are stronger because they combine many trees, reducing overfitting.
- Ensemble methods often outperform single models.

ğŸ“Š Metrics Learned
- Entropy & Information Gain â†’ used to build trees.
- Accuracy â†’ measures how well the tree/forest predicts.

ğŸ“ Reflections
- Decision trees felt natural and easy to understand.
- Random forests showed how combining models can improve performance.
- Visualizing the tree splits made the concept much clearer.
- Learned that ensemble methods are powerful in real-world ML tasks.
