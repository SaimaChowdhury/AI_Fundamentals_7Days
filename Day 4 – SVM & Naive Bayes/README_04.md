**ğŸ“˜ Day 4 â€“ Support Vector Machines (SVMs)**

ğŸ“– Concepts Learned
- Support Vector Machines (SVMs): Classification models that find the best boundary between classes.
- Hyperplane: The decision boundary (line in 2D, plane in higher dimensions).
- Support Vectors: The closest data points to the boundary that define it.
- Margin: The distance between the boundary and the nearest points; SVM maximizes this margin.
- Kernel Trick: A method to transform data into higher dimensions to make it separable.

ğŸ§ª Examples & Practice
- Implemented a Linear SVM on a toy dataset to classify points.
- Visualized the decision boundary and support vectors.
- Experimented with Nonâ€‘Linear SVM using RBF kernel for overlapping classes.
- Compared performance between linear and kernel SVMs.

ğŸ”‘ Key Takeaways
- SVMs aim to maximize the margin, making boundaries more robust.
- Support vectors are critical â€” they alone define the boundary.
- Kernels allow SVMs to handle complex, nonâ€‘linear data.
- Visualization helps in understanding how SVM separates classes.

ğŸ“Š Metrics Learned
- Accuracy â†’ overall correctness of predictions.
- Precision & Recall â†’ useful for imbalanced datasets.

ğŸ“ Reflections
- SVMs felt like drawing the â€œbest possible lineâ€ between groups.
- Support vectors showed me that not all points matter equally â€” only the closest ones do.
- Kernel trick was eyeâ€‘opening: itâ€™s like magic that makes impossible separations possible.
- Visualizing boundaries made the math much easier to grasp.


